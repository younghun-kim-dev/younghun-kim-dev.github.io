<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="overview">Overview</h2> <ul> <li> <strong>Goal.</strong> Understand under what memory-system and sharing conditions a SHA-3 RoCC retains its ~170–200× speedup as messages grow from cache-sized to hundreds of KB.</li> <li> <strong>Timeline.</strong> Apr. 2025 – Jun. 2025 (independent project).</li> <li> <strong>Stack.</strong> Chipyard (Rocket / BOOM + RoCC), Verilator + DRAMSim2, custom benchmarking scripts in <code class="language-plaintext highlighter-rouge">sims/verilator/</code>.</li> </ul> <p>A hardware-wallet security failure pushed me toward a simple question:</p> <blockquote> <p>If an accelerator advertises a 200× speedup, under what memory conditions does that speedup actually survive?</p> </blockquote> <p>This project takes SHA-3 as a concrete case and shows that a single-bank inclusive L2 can silently turn a “~170× accelerator” into a “120× accelerator” at scale, and that a multi-bank L2 almost completely recovers the loss.</p> <hr> <h2 id="research-question">Research question</h2> <p>As message size scales from <strong>136 B → 557 KB</strong>, I asked:</p> <ol> <li><strong>When does the SHA-3 RoCC keep its headline speedups (≈170–200×)?</strong></li> <li><strong>When and why do speedups collapse, even though the accelerator core is unchanged?</strong></li> <li><strong>Which part of the memory path (L1, TLB, L2 banking, DRAM channels) is responsible?</strong></li> </ol> <hr> <h2 id="methodology--setup">Methodology &amp; setup</h2> <ul> <li> <strong>SoC &amp; configs.</strong> <ul> <li>Integrated a SHA-3 RoCC into Rocket and mixed Rocket+BOOM SoCs.</li> <li>Defined a config matrix in Chipyard: <ul> <li> <strong>Baseline.</strong> Single-bank inclusive L2 with SHA-3 + Rocket/BOOM.</li> <li> <strong>Variants.</strong> 1/2/4/8-bank L2, SW-only baselines, extra memory channels.</li> </ul> </li> </ul> </li> <li> <strong>Benchmark pipeline.</strong> <ul> <li>Two binaries per config: <ul> <li> <code class="language-plaintext highlighter-rouge">sha3-rocc.riscv</code> – SHA-3 via RoCC accelerator.</li> <li> <code class="language-plaintext highlighter-rouge">sha3-sw.riscv</code> – software-only SHA-3 on the same core.</li> </ul> </li> <li>For each size, log: <ul> <li> <code class="language-plaintext highlighter-rouge">hw_cycles</code>, <code class="language-plaintext highlighter-rouge">sw_cycles</code>, <code class="language-plaintext highlighter-rouge">speedup = sw_cycles / hw_cycles</code>.</li> </ul> </li> </ul> </li> <li> <strong>Message sizes.</strong> <ul> <li>Swept <strong>136 × 2ᵖ B</strong> for p = 0…12: <ul> <li>136, 272, …, 557,056 bytes.</li> </ul> </li> </ul> </li> <li> <strong>Automation.</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">sha3_compare.sh</code> runs HW vs SW once and emits <code class="language-plaintext highlighter-rouge">sha3_speed.csv</code>.</li> <li> <code class="language-plaintext highlighter-rouge">gather_sha3_data.sh</code> aggregates per-size CSVs into <code class="language-plaintext highlighter-rouge">combined_sha3_speed.csv</code> used to plot the speedup curve.</li> </ul> </li> </ul> <p>This keeps the experiment fully reproducible from the repo: new configs can reuse the same scripts to generate new curves.</p> <hr> <h2 id="key-findings">Key findings</h2> <h3 id="1-plateau-then-collapse-with-a-single-bank-inclusive-l2">1. Plateau then collapse with a single-bank inclusive L2</h3> <p>With a <strong>single-bank inclusive L2</strong>, speedup behaves as follows:</p> <ul> <li> <strong>Small–mid sizes (≈2–70 KB).</strong> <ul> <li>Speedup stays in a <strong>stable ~168–176× band</strong>.</li> <li>The accelerator is well-fed; memory is not yet the bottleneck.</li> </ul> </li> <li> <strong>Medium sizes (≈140–280 KB).</strong> <ul> <li>Speedup drifts down to <strong>~145–160×</strong>.</li> <li>Working set begins to stress L2 capacity / associativity.</li> </ul> </li> <li> <strong>Largest size (557,056 B).</strong> <ul> <li>Speedup <strong>collapses to 120.27×</strong>.</li> <li>Cycle-accurate traces show the core is not saturated; instead: <ul> <li>misses, refills, and evictions serialize on the single L2 bank,</li> <li>coherence and refill traffic back up behind a narrow bottleneck.</li> </ul> </li> </ul> </li> </ul> <p>So from the system’s point of view, the same RoCC looks like:</p> <ul> <li> <strong>≈170×</strong> at mid-size messages, but only</li> <li> <strong>~120×</strong> at application-scale messages.</li> </ul> <p>The difference is entirely in the memory path.</p> <h3 id="2-multi-bank-inclusive-l2-recovers-34-throughput-at-scale">2. Multi-bank inclusive L2 recovers ~34% throughput at scale</h3> <p>Redesigning the inclusive L2 as a <strong>multi-bank cache</strong> (with more effective capacity and sub-banking concurrency):</p> <ul> <li>At <strong>557,056 B</strong>: <ul> <li>Single-bank L2: <code class="language-plaintext highlighter-rouge">speedup ≈ 120.27×</code>.</li> <li>Multi-bank L2: <code class="language-plaintext highlighter-rouge">speedup ≈ 160.90×</code>.</li> </ul> </li> <li>This is roughly a <strong>+34% improvement in throughput</strong>, and the new point at 557 KB sits <strong>back near the ~170× plateau</strong> seen at mid sizes.</li> </ul> <p>The SHA-3 core and ISA are unchanged; only L2 structure is different.</p> <p><strong>Interpretation.</strong></p> <ul> <li>“Speedup” is meaningless without a <strong>memory-path contract</strong>.</li> <li>At realistic message sizes, the platform advertised “170×” but delivered “120×” purely because of L2 banking.</li> <li>Once L2 is redesigned, the same accelerator again behaves like the promised high-speedup unit.</li> </ul> <hr> <h2 id="what-i-learned">What I learned</h2> <ul> <li>Accelerator evaluation must treat <strong>memory hierarchy as part of the accelerator</strong>.</li> <li>A single number (“206× speedup”) hides the structure of the curve; profiling across sizes is essential.</li> <li>Multi-bank L2 design and concurrency matter as much as the RoCC datapath for real-world throughput.</li> </ul> <p>This project directly seeded my later work on Gemmini and heterogeneous SoCs, where I treated the memory subsystem and scheduler as first-class levers for stabilizing accelerator performance.</p> <hr> <h2 id="code--data-map">Code &amp; data map</h2> <p>Key locations in the <code class="language-plaintext highlighter-rouge">chipyard_sha3</code> branch:</p> <ul> <li> <strong>Configs.</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">generators/chipyard/src/main/scala/config/AccelMemSweep.scala</code><br> SHA-3 + Rocket/BOOM configs; 1/2/4/8-bank L2, SW-only baselines.</li> <li> <code class="language-plaintext highlighter-rouge">generators/chipyard/src/main/scala/config/RocketSha3Configs.scala</code><br> Rocket-only SHA-3 configs, including 4-bank L2 + extra memory channels.</li> </ul> </li> <li> <strong>Benchmarks.</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">generators/sha3</code> (forked submodule) – SHA-3 RoCC implementation.</li> <li> <code class="language-plaintext highlighter-rouge">sims/verilator/sha3_compare.sh</code> – run HW vs SW once and emit <code class="language-plaintext highlighter-rouge">sha3_speed.csv</code>.</li> <li> <code class="language-plaintext highlighter-rouge">sims/verilator/gather_sha3_data.sh</code> – sweep sizes and aggregate into <code class="language-plaintext highlighter-rouge">combined_sha3_speed.csv</code>.</li> </ul> </li> <li> <strong>Results &amp; plots.</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">sims/verilator/**/</code> – per-size logs (<code class="language-plaintext highlighter-rouge">*.log</code>, <code class="language-plaintext highlighter-rouge">hw.cycs</code>, <code class="language-plaintext highlighter-rouge">sw.cycs</code>, CSVs).</li> <li> <code class="language-plaintext highlighter-rouge">figs/sha3_speedup_vs_size_with_arrow_colored.png</code> – speedup curve with the 557 KB improvement highlighted.</li> </ul> </li> </ul> </body></html>