<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Independent Chipyard project investigating <strong>how shared-memory contention shapes accelerator behavior</strong> when workloads co-run on a heterogeneous RISC-V SoC (BOOM, 2×Rocket, Gemmini). I built a <strong>co-run profiling framework</strong> with a configurable contention generator and per-tile logging, showed that DRAM saturation and L2 bank contention can slow a 256×256 GEMM by ≈<strong>3×</strong>, and then used <strong>memory-topology tuning + phase-aware scheduling</strong> to recover up to <strong>2.7×</strong> throughput and tighten tile-level latency bands.</p> <hr> <h2 id="key-questions">Key questions</h2> <ul> <li>When BOOM+Gemmini share DRAM and L2 with CPU cores, what actually causes <strong>multi× slowdowns</strong>?</li> <li>Do slowdowns look random, or do they follow <strong>structured phases</strong> tied to memory topology and stress patterns?</li> <li>Can we restore throughput and <strong>predictability</strong> with coordinated HW/SW changes, without touching Gemmini’s MAC array?</li> </ul> <hr> <h2 id="co-run-profiling-framework">Co-run profiling framework</h2> <p><img src="/assets/img/fig1_gemm_latency.png" alt="GEMM latency vs memory topology and stress"></p> <ul> <li> <strong>SoC topology (Chipyard configs)</strong> <ul> <li>BOOM + 2×Rocket + Gemmini with variants like: <ul> <li> <code class="language-plaintext highlighter-rouge">GemminiLargeBoomV4Rocket2Config</code> – 1 DRAM channel, default L2.</li> <li> <code class="language-plaintext highlighter-rouge">GemminiLargeBoomV4Rocket22CHConfig</code> – 2 DRAM channels, default L2.</li> <li> <code class="language-plaintext highlighter-rouge">GemminiLargeBoomV4Rocket22CHL24BanksConfig</code> – 2 DRAM channels + 4-bank L2, tuned controller.</li> </ul> </li> </ul> </li> <li> <strong>Role assignment</strong> <ul> <li>Hart 0 – Rocket #1: memory-bandwidth stressor.</li> <li>Hart 1 – Rocket #2: independent memory-bandwidth stressor.</li> <li>Hart 2 – BOOM: Gemmini host running GEMM kernels.</li> </ul> </li> <li> <strong>Benchmarks</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">hetero_gemm_bwtest-baremetal</code> <ul> <li>One-shot Gemmini GEMM (256×256×256) co-running with two Rocket linear “mem-stress” loops.</li> <li>Sweeps stress sizes (2 / 8 / 16 MiB) and records per-hart completion.</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">hetero_gemm_bwtest2-baremetal</code> <ul> <li>Same co-run setup, but splits the GEMM into <strong>16× 64×64 tiles</strong> and logs <strong>per-tile cycles</strong>.</li> </ul> </li> </ul> </li> <li> <strong>Simulator</strong> <ul> <li>Verilator harness + DRAMSim2 to model realistic DRAM timing, with large max-cycles to survive heavy stress.</li> </ul> </li> </ul> <hr> <h2 id="one-shot-gemm-under-co-run-stress">One-shot GEMM under co-run stress</h2> <ul> <li> <strong>Workload</strong> <ul> <li>Gemmini WS GEMM: <strong>256×256×256</strong>.</li> <li>Two Rocket harts each streaming <strong>2 / 8 / 16 MiB</strong> from DRAM (stride 64).</li> </ul> </li> <li> <strong>Behavior</strong> <ul> <li>Baseline (almost no stress): <strong>≈180k cycles</strong>.</li> <li>Single-channel DRAM + default L2, <strong>8–16 MiB</strong> stress: <ul> <li>GEMM latency jumps to <strong>≈552k cycles</strong> → <strong>3.07× slowdown</strong>.</li> <li>Increasing stress beyond 8 MiB doesn’t change latency → <strong>fully bandwidth-limited</strong>.</li> </ul> </li> <li>Adding a <strong>second DRAM channel alone</strong> only reduces slowdown slightly (~2.89×).</li> </ul> </li> <li> <strong>Topology + controller tuning</strong> <ul> <li>With <strong>2 DRAM channels + 4 L2 banks + tuned controller</strong>: <ul> <li>8 MiB stress run recovers to <strong>≈207k cycles</strong> (≈<strong>1.15×</strong> vs baseline).</li> <li>This is ≈<strong>2.7× faster</strong> than the 1-channel/8 MiB case, with the <strong>same Gemmini compute array</strong>.</li> </ul> </li> </ul> </li> </ul> <hr> <h2 id="tile-level-predictability">Tile-level predictability</h2> <p><img src="/assets/img/fig2_tile_latency.png" alt="Tile-latency distributions under contention"></p> <ul> <li> <strong>Setup</strong> <ul> <li>Compare: <ul> <li>1CH + default L2 vs</li> <li>2CH + 4-bank L2 (tuned controller),</li> </ul> </li> <li>Both under <strong>8 MiB</strong> co-run stress.</li> <li>GEMM split into <strong>16 tiles (64×64)</strong>; each tile’s cycles are logged.</li> </ul> </li> <li> <strong>Results</strong> <ul> <li> <strong>1CH + default L2</strong> <ul> <li>Mean tile latency ≈ <strong>40.5k cycles</strong>, std-dev ≈ <strong>3.8k cycles</strong>.</li> <li>First tile can hit <strong>&gt;50k cycles</strong>; distribution is slow and jittery.</li> </ul> </li> <li> <strong>2CH + 4-bank L2</strong> <ul> <li>Mean tile latency ≈ <strong>13.7k cycles</strong>, std-dev ≈ <strong>0.95k cycles</strong>.</li> <li>All tiles sit in a <strong>tight band</strong> around the mean.</li> </ul> </li> </ul> </li> <li> <strong>Interpretation</strong> <ul> <li>The tuned memory topology does more than increase average throughput: <ul> <li>It keeps Gemmini tile latency in a <strong>narrow, predictable band</strong>, even under strong co-run contention.</li> <li>This directly matches my broader goal: <strong>accelerators that keep their promises under shared-memory constraints</strong>.</li> </ul> </li> </ul> </li> </ul> <hr> <h2 id="takeaways">Takeaways</h2> <ul> <li>Co-run slowdowns are <strong>structured, not random</strong>: <ul> <li>DRAM saturation and L2 bank contention appear in predictable phases.</li> </ul> </li> <li>Small, coordinated <strong>HW/SW changes</strong>—more DRAM channels, L2 banking, tuned controller, phase-aware scheduling—can: <ul> <li>Recover <strong>up to 2.7×</strong> throughput, and</li> <li>Dramatically tighten <strong>latency variation</strong> across tiles.</li> </ul> </li> <li>This project generalizes the earlier SHA-3 and Gemmini work to <strong>true heterogeneous, multi-workload settings</strong>, pushing my research from “single-accelerator speedups” toward <strong>system-level, memory-centric guarantees</strong>.</li> </ul> </body></html>