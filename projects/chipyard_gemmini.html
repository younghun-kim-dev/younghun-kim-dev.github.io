<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Independent Gemmini follow-up project testing whether <strong>matrix GEMM workloads share the same memory-path limits</strong> I saw in SHA-3. I built a <strong>WS/OS GEMM framework</strong> comparing CPU and Gemmini across matrix sizes, identified two regimes (overhead-dominated vs bandwidth-limited), and co-designed <strong>scratchpad banking, bus width, and DMA alignment</strong>. The optimized memory pipeline improves <strong>1024³</strong> throughput by ≈<strong>58%</strong> and lowers the <strong>CPU↔Gemmini offload threshold K*</strong>, validated on FireSim (AWS EC2 F1).</p> <hr> <h2 id="key-questions">Key questions</h2> <ul> <li>For GEMM, when is it actually worth offloading to Gemmini instead of staying on the CPU?</li> <li>Do GEMM workloads exhibit the same <strong>“compute is cheap, data movement sets the pace”</strong> behavior as SHA-3?</li> <li>How much of the missing performance is in <strong>Gemmini’s memory path</strong> (SPM/ACC, buses, DMA), not its MAC array?</li> </ul> <hr> <h2 id="part-1--rocketconfig-wsos-baseline">Part 1 – RocketConfig WS/OS baseline</h2> <p><img src="/assets/img/gemminirocket_mac100_vs_M.png" alt="MAC/100cyc vs matrix size (Rocket + Gemmini)"></p> <ul> <li> <strong>Setup</strong> <ul> <li>SoC: <strong>GemminiRocketConfig</strong> (Rocket core + Gemmini).</li> <li>Sweep <strong>M = N = K ∈ {8, …, 2048}</strong>, with <strong>WS</strong> and <strong>OS</strong> dataflows.</li> <li>Metrics: cycles, <strong>MAC/100cyc</strong>, correctness checks.</li> </ul> </li> <li> <strong>Findings</strong> <ul> <li>Clear <strong>two-regime behavior</strong>: <ul> <li>Small matrices (≤32): dominated by <strong>launch + data-movement overheads</strong>.</li> <li>Large matrices (≥256): <strong>MAC/100cyc plateaus</strong> → <strong>bandwidth / tiling</strong> dominate.</li> </ul> </li> <li> <strong>WS &gt; OS</strong> across all sizes in both latency and MAC utilization.</li> <li>This curve becomes the <strong>baseline</strong> for later BOOM+Gemmini and memory-path changes.</li> </ul> </li> </ul> <hr> <h2 id="part-2--boomgemmini-memory-pipeline-co-design">Part 2 – BOOM+Gemmini memory-pipeline co-design</h2> <p><img src="/assets/img/boomgemmini_mac100_before_after.png" alt="WS/OS MAC/100cyc – before vs after memory-pipeline optimization"></p> <ul> <li> <strong>Setup</strong> <ul> <li>SoCs: <ul> <li> <strong>Before:</strong> <code class="language-plaintext highlighter-rouge">GemminiLargeBoomV4Config</code>.</li> <li> <strong>After:</strong> <code class="language-plaintext highlighter-rouge">GemminiLargeBoomV43Config</code>.</li> </ul> </li> <li>Changes implemented as Gemmini/Chipyard <strong>config mixins</strong>: <ul> <li>Scratchpad/accumulator (<strong>SPM/ACC</strong>) banking &amp; sizing.</li> <li>System-bus beat width.</li> <li>Gemmini <strong>DMA bus-width alignment</strong>.</li> </ul> </li> <li>Workload: one-shot GEMM with <strong>M = N = K ∈ {8, …, 1024}</strong>, WS/OS.</li> </ul> </li> <li> <p><strong>Results (selected MAC/100cyc)</strong></p> <ul> <li>At <strong>1024³</strong>: <ul> <li>WS: <strong>16,047 → 25,433 MAC/100cyc</strong> (≈<strong>+58%</strong> throughput).</li> <li>OS: <strong>13,828 → 17,004 MAC/100cyc</strong> (≈<strong>+23%</strong> throughput).</li> </ul> </li> <li>Similar gains at 256–512³ show that the gap is <strong>systematic</strong>, not a one-off artifact.</li> </ul> </li> <li> <strong>Interpretation</strong> <ul> <li> <strong>Same MAC array, different memory path</strong>: <ul> <li>The “missing” performance was sitting in <strong>SPM/ACC layout, bus width, and DMA alignment</strong>.</li> <li>Memory-centric co-design raises Gemmini into a <strong>higher-throughput plateau</strong> without changing compute.</li> </ul> </li> </ul> </li> </ul> <hr> <h2 id="part-3--cpu-vs-gemmini-offload-threshold-k">Part 3 – CPU vs Gemmini offload threshold K*</h2> <ul> <li> <strong>Offload-threshold pipeline</strong> <ul> <li>On a BOOM+Gemmini SoC, I built a <strong>CPU vs Gemmini threshold benchmark</strong>: <ul> <li>Sweep tile shapes <strong>(M, N)</strong> and depths <strong>K ∈ {1,…,8}</strong>.</li> <li>Compare blocked CPU GEMM vs Gemmini (OS) cycles.</li> <li>Define <strong>K*(M, N)</strong> = smallest K where Gemmini beats the CPU.</li> </ul> </li> </ul> </li> <li> <strong>Empirical behavior</strong> <ul> <li>Tiny tiles like <strong>(4,4)</strong> and <strong>(8,4)</strong> never beat the CPU for <strong>K ≤ 8</strong><br> → launch &amp; data-movement overhead dominate.</li> <li>Larger, near-square tiles like <strong>(10,10)</strong> and <strong>(12,12)</strong> already win at <strong>K* = 1</strong>.</li> <li>Shape matters: <ul> <li>(8,12) has <strong>K* = 1</strong>, while (12,8) has <strong>K* = 5</strong><br> → <strong>loop/blocking order and memory layout</strong> strongly affect offload decisions.</li> </ul> </li> </ul> </li> <li> <strong>Speedups when offloaded</strong> <ul> <li>When offloading is beneficial (Offload = 1): <ul> <li>Median speedup ≈ <strong>1.7×</strong>, mean ≈ <strong>1.9×</strong>, best cases ≈ <strong>4×</strong>.</li> </ul> </li> </ul> </li> </ul> <hr> <h2 id="takeaways">Takeaways</h2> <ul> <li>GEMM confirms the same pattern I saw in SHA-3: <ul> <li><strong>Compute is cheap; data movement and overheads decide real speedup.</strong></li> </ul> </li> <li>Offloading is <strong>not “always good”</strong>: <ul> <li>There is a real, measurable <strong>offload threshold K*(M, N)</strong> that shifts with the memory path.</li> </ul> </li> <li>This project turns that intuition into <strong>measurement infrastructure</strong>: <ul> <li>For any new Gemmini/SoC config, re-running the pipeline shows how memory-centric changes move K* and reshape the performance envelope.</li> </ul> </li> </ul> </body></html>